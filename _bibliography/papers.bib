---
---

@misc{tumma2023leveraging,
      title={Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control}, 
      author={Neehal Tumma and Mathias Lechner and Noel Loo and Ramin Hasani and Daniela Rus},
      year={2023},
      eprint={2310.03915},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{chiu-etal-2022-joint,
    title = {A Joint Learning Approach for Semi-supervised Neural Topic Modeling},    author = {Chiu*, Jeffrey  and
      Mittal*, Rajat  and
      Tumma*, Neehal  and
      Sharma, Abhishek  and
      Doshi-Velez, Finale},
    booktitle = {Proceedings of the Sixth Workshop on Structured Prediction for NLP},
    year = {2022},
    publisher = {Association for Computational Linguistics},
    pdf = {https://aclanthology.org/2022.spnlp-1.5},
    pages = {40--51},
    abstract = {Topic models are some of the most popular ways to represent textual data in an interpret-able manner. Recently, advances in deep generative models, specifically auto-encoding variational Bayes (AEVB), have led to the introduction of unsupervised neural topic models, which leverage deep generative models as opposed to traditional statistics-based topic models. We extend upon these neural topic models by introducing the Label-Indexed Neural Topic Model (LI-NTM), which is, to the extent of our knowledge, the first effective upstream semi-supervised neural topic model. We find that LI-NTM outperforms existing neural topic models in document reconstruction benchmarks, with the most notable results in low labeled data regimes and for data-sets with informative labels; furthermore, our jointly learned classifier outperforms baseline classifiers in ablation studies.}
}
